{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_Numpy",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPS+HFfjskHe6vOSsLcSXVW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LokeshVadlamudi/DeepLearningClass/blob/master/MNIST_Numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "369nE8DDs0Eg",
        "colab_type": "text"
      },
      "source": [
        "MNIST classifier using numpy and python without CNN and just using plain neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ez2wGNMI-8Z",
        "colab_type": "text"
      },
      "source": [
        "Things to do:\n",
        "* The code should do mini batch gradient descent along with appropriate learning rate\n",
        "* The code should do dropout - try various dropout rates and pick the one which works well.\n",
        "* The code should initialize the random weights of network properly\n",
        "* The code should do basic image augmentations to supplement the training data (not testing data) using keras libraries  (NEW than the deck)\n",
        "*  The code should use  3 or more layers for training (not 2 as in example ) - you have to tune and pick number of neurons in your layer and number of layers\n",
        "*  The code will continue to use relu activation layer in right places like python code\n",
        "* The code should normalize the input as discussed in the class before training (scaling the input)\n",
        "* The code should use appropriate learning rate (try out few to find out which one works) - you can use adaptive learning rates like different learning rates per epoch or per mini batch\n",
        "* The code should provide appropriate metrics, visualization,  testing and training accuracy etc.,. and plot the results and confusion matrix  (this is important)\n",
        "* The code should display top common errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKsNhj4vFOD6",
        "colab_type": "text"
      },
      "source": [
        "Importing the modules we need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG82S44SDTgg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "467ae207-49eb-4483-8f37-c442604efe58"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import trange\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJqr7DXpOj2n",
        "colab_type": "text"
      },
      "source": [
        "Lets Create a Layer Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHQoRZ7LOpqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer:\n",
        "  def __init__(self):\n",
        "    self.wgts = np.zeros(shape=(input.shape[1], 10))\n",
        "    bs = np.zeros(shape=(10,))\n",
        "\n",
        "  def forward(self, ip):\n",
        "        op = np.matmul(ip, self.wgts) + bs\n",
        "        return op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brdJrWrnFzjD",
        "colab_type": "text"
      },
      "source": [
        "Creating Dense Layer Class , we can give learning rate as input to our dense layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtxTR787PXiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DenseLayer(Layer):\n",
        "    def __init__(self, ips, ops, learning_rate=0.1):\n",
        "        # using normal distribution and initializing small random weights\n",
        "        self.wgts = np.random.randn(ips, ops)*0.01\n",
        "        self.bs = np.zeros(ops)\n",
        "        self.learning_rate = learning_rate\n",
        "    def forward(self,input):\n",
        "        return np.matmul(input, self.wgts) + self.bs\n",
        "    def backward(self,ip,grad_op):\n",
        "        grad_ip = np.dot(grad_op,np.transpose(self.wgts))\n",
        "        grad_wgts = np.transpose(np.dot(np.transpose(grad_op),ip))\n",
        "        grad_bs = np.sum(grad_op, axis = 0)\n",
        "        \n",
        "        # gradient descent method(stochastic)\n",
        "        self.bs = self.bs - self.learning_rate * grad_bs\n",
        "        self.wgts = self.wgts - self.learning_rate * grad_wgts\n",
        "        return grad_ip\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BskHNC8oYJf",
        "colab_type": "text"
      },
      "source": [
        "Creating Activation Function Relu Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTFO73WwKljV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReLuLayer(Layer):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def backward(self, ip, grad_op):\n",
        "        relu_g = ip > 0\n",
        "        return grad_op*relu_g \n",
        "    \n",
        "    def forward(self, ip):\n",
        "        return np.maximum(0,ip)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujYF2r37IoYG",
        "colab_type": "text"
      },
      "source": [
        "Lets define a loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoEPifeoKqhv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softcentropy(lgs,ref_ans):\n",
        "    log_ans = lgs[np.arange(len(lgs)),ref_ans]\n",
        "    \n",
        "    finalEnt = - log_ans + np.log(np.sum(np.exp(lgs),axis=-1))\n",
        "    \n",
        "    return finalEnt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7N7A12uKsvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grad_softcentropy(lgs,ref_ans):\n",
        "    oneans = np.zeros_like(lgs)\n",
        "    oneans[np.arange(len(lgs)),ref_ans] = 1 \n",
        "    smax = np.exp(lgs) / np.exp(lgs).sum(axis=-1,keepdims=True) \n",
        "    return (- oneans + smax) / lgs.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tm3EyrFofs_",
        "colab_type": "text"
      },
      "source": [
        "import the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHWrQUIUI1ha",
        "colab_type": "text"
      },
      "source": [
        "Normalizing the inputs and creating validation sets also"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv9exEVEKu_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def dataLoad(flatten=False):\n",
        "    #loading dataset from the keras library\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "    # normalizing X\n",
        "    x_train = x_train.astype(float) / 255.\n",
        "    x_test = x_test.astype(float) / 255.\n",
        "    # creating validation set\n",
        "    x_train, x_val = x_train[:-10000], x_train[-10000:]\n",
        "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
        "    if flatten:\n",
        "        x_train = x_train.reshape([x_train.shape[0], -1])\n",
        "        x_val = x_val.reshape([x_val.shape[0], -1])\n",
        "        x_test = x_test.reshape([x_test.shape[0], -1])\n",
        "    return x_train, y_train, x_val, y_val, x_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOninHY_mWn-",
        "colab_type": "text"
      },
      "source": [
        "Loading the DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCazZhgMKzS0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "71b83048-8653-4894-d67a-189fe507c45d"
      },
      "source": [
        "x_train, y_train, x_val, y_val, x_test, y_test = dataLoad(flatten=True)    "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0S8qi9BmPiD",
        "colab_type": "text"
      },
      "source": [
        "Lets do Data Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3G1_ZOEmI9_",
        "colab_type": "code",
        "outputId": "71127a7c-0fd4-4e08-99c8-35349121e29f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "(X_Train, Y_train), (X_test, Y_test) = keras.datasets.cifar10.load_data()\n",
        "datamodder = ImageDataGenerator(\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=20,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "datamodder.fit(X_Train)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 13s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:348: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, which overrides setting of `featurewise_center`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0Bdv97oI8QA",
        "colab_type": "text"
      },
      "source": [
        "Creating the network of 3 layers, each layer coupled with relu activation layer, also giving different learning rates for different layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L57B6MKDmH1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating a initial list to hold our layers.\n",
        "neuralnet = []\n",
        "\n",
        "#first input layer\n",
        "neuralnet.append(DenseLayer(x_train.shape[1],100))\n",
        "neuralnet.append(ReLuLayer())\n",
        "#first Hidden Layer\n",
        "neuralnet.append(DenseLayer(100,200,learning_rate=0.2))\n",
        "neuralnet.append(ReLuLayer())\n",
        "#second Hidden Layer\n",
        "neuralnet.append(DenseLayer(200,200,learning_rate=0.1))\n",
        "neuralnet.append(ReLuLayer())\n",
        "#final output layer\n",
        "neuralnet.append(DenseLayer(200,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQtuRpNIXk30",
        "colab_type": "text"
      },
      "source": [
        "passing the neural network we created previously as a input and iterating through all the layers and calling forward function on each layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgnpHHVDK3N_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(neuralnet, x):\n",
        "    ip = x\n",
        "    acts = []\n",
        "    for i in range(len(neuralnet)):\n",
        "        acts.append(neuralnet[i].forward(x))\n",
        "        x = neuralnet[i].forward(x)\n",
        "    assert len(acts) == len(neuralnet)\n",
        "    return acts\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NePub8aiYSNN",
        "colab_type": "text"
      },
      "source": [
        "Predicting neural net predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg2DVwp9LAeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(neuralnet,x):\n",
        "    lgs = forward(neuralnet,x)[-1]\n",
        "    return lgs.argmax(axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkqsv1pNYqVO",
        "colab_type": "text"
      },
      "source": [
        "Now, we will train our network batch by batch x and y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlZnyYCmLClG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(nn,x,y):\n",
        "    \n",
        "    # getting layer activations \n",
        "    layer_acts = forward(nn,x)\n",
        "    lgs = layer_acts[-1]\n",
        "    \n",
        "    #lets compute init gradient and loss\n",
        "    loss = softcentropy(lgs,y)\n",
        "    l_grad = grad_softcentropy(lgs,y)\n",
        "    for i in range(1, len(nn)):\n",
        "        l_grad = nn[len(nn) - i].backward(layer_acts[len(nn) - i - 1], l_grad)\n",
        "    return np.mean(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXppCiNxJnu4",
        "colab_type": "text"
      },
      "source": [
        "Mini Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gSl9Wj0LE08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def miniBatchMachineGun(ips, goals, sizeBatch, shuffle=False):\n",
        "    assert len(ips) == len(goals)\n",
        "    if shuffle:\n",
        "        indexes = np.random.permutation(len(ips))\n",
        "    for first_idx in trange(0, len(ips) - sizeBatch + 1, sizeBatch):\n",
        "        if shuffle:\n",
        "            ext = indexes[first_idx:first_idx + sizeBatch]\n",
        "        else:\n",
        "            ext = slice(first_idx, first_idx + sizeBatch)\n",
        "        yield ips[ext], goals[ext]\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjxSeUFzpnuP",
        "colab_type": "text"
      },
      "source": [
        "Final training of the data and displaying the results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB0PaFqdLHvY",
        "colab_type": "code",
        "outputId": "e923b6ed-ae6a-4513-ba2f-c08bc2839491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        }
      },
      "source": [
        "#two lists to hold the values of each cycle\n",
        "training_data_logger = []\n",
        "\n",
        "validation_data_logger = []\n",
        "\n",
        "for eachCycle in range(5):\n",
        "\n",
        "    #we are calling minibatch function that gives us batches of training data.\n",
        "\n",
        "    for batchX,batchY in miniBatchMachineGun(x_train,y_train,sizeBatch=32,shuffle=True):\n",
        "\n",
        "        train(neuralnet,batchX,batchY)\n",
        "    \n",
        "    #appending each cycle results accuracies to the lists\n",
        "    training_data_logger.append(np.mean(predict(neuralnet,x_train)==y_train))\n",
        "\n",
        "    validation_data_logger.append(np.mean(predict(neuralnet,x_val)==y_val))\n",
        "    \n",
        "    #printing the data results\n",
        "    print(\"\\n Round\",eachCycle)\n",
        "    print(\"\\n Train accuracy:\",training_data_logger[-1])\n",
        "    print(\"\\n Val accuracy:\",validation_data_logger[-1])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1562/1562 [00:04<00:00, 353.81it/s]\n",
            "  2%|▏         | 36/1562 [00:00<00:04, 355.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Round 0\n",
            "\n",
            " Train accuracy: 0.98984\n",
            "\n",
            " Val accuracy: 0.9551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1562/1562 [00:04<00:00, 359.48it/s]\n",
            "  2%|▏         | 38/1562 [00:00<00:04, 369.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Round 1\n",
            "\n",
            " Train accuracy: 0.99352\n",
            "\n",
            " Val accuracy: 0.9577\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1562/1562 [00:04<00:00, 365.92it/s]\n",
            "  2%|▏         | 35/1562 [00:00<00:04, 346.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Round 2\n",
            "\n",
            " Train accuracy: 0.99296\n",
            "\n",
            " Val accuracy: 0.9565\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1562/1562 [00:04<00:00, 360.88it/s]\n",
            "  2%|▏         | 38/1562 [00:00<00:04, 372.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Round 3\n",
            "\n",
            " Train accuracy: 0.99136\n",
            "\n",
            " Val accuracy: 0.9514\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1562/1562 [00:04<00:00, 362.34it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Round 4\n",
            "\n",
            " Train accuracy: 0.99188\n",
            "\n",
            " Val accuracy: 0.9534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAVgOmawr1i4",
        "colab_type": "text"
      },
      "source": [
        "Finally, lets get confusion matrix metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc9yWgO5LLUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c9396681-d594-4099-9e41-4bf9915027e4"
      },
      "source": [
        "y_pred = predict(neuralnet,x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, ..., 4, 5, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcDfrQjIt3Gj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "2b9cc335-7cb5-4b53-8305-a9bdf820f478"
      },
      "source": [
        "y_pred.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRFRj0teus1X",
        "colab_type": "text"
      },
      "source": [
        "#confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qKLx18at3cr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "2c07bb07-8ef0-45ed-c7d0-d768f1a25f8b"
      },
      "source": [
        "confusion_matrix(y_test,y_pred)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 954,    0,    1,    2,    0,    3,   15,    1,    4,    0],\n",
              "       [   0, 1120,    3,    2,    1,    0,    3,    2,    4,    0],\n",
              "       [   7,    2,  979,   10,    4,    1,    6,    3,   19,    1],\n",
              "       [   1,    0,   12,  961,    0,   14,    3,    3,   13,    3],\n",
              "       [   2,    2,    4,    0,  937,    0,   20,    1,    2,   14],\n",
              "       [   6,    0,    2,   26,    4,  828,   10,    0,   10,    6],\n",
              "       [   8,    3,    1,    0,    8,    4,  931,    0,    3,    0],\n",
              "       [   3,    8,   22,    5,    4,    1,    0,  966,    4,   15],\n",
              "       [   4,    0,    8,    6,    5,    3,   14,    4,  929,    1],\n",
              "       [   7,    4,    1,    8,   26,    7,    1,   14,   12,  929]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TARthrZBuiVe",
        "colab_type": "text"
      },
      "source": [
        "#accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr6c2PxAuB9g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "d280b1cc-79ae-4420-a780-cece4439efa8"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9534"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0774XPnunVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}